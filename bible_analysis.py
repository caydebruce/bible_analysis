# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUMXjWzlEc7T0wGCqOe66duFVhyfDTFw
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

data_folder = '/content/drive/MyDrive/bible_project/archive/'

# List of CSV files we want to parse
translation_files = ["t_kjv.csv", "t_asv.csv", "t_bbe.csv", "t_ylt.csv", "t_wbt.csv", "t_web.csv"]

bible_dict = {}

for filename in translation_files:
    translation = os.path.splitext(filename)[0]

    csv_path = os.path.join(data_folder, filename)
    df = pd.read_csv(csv_path)

    # Create an empty sub-dictionary for this translation
    bible_dict[translation] = {}

    # Loop over each row in the CSV
    for _, row in df.iterrows():
        book    = row['b']
        chapter = int(row['c'])
        verse   = int(row['v'])
        text    = row['t']

        if book not in bible_dict[translation]:
            bible_dict[translation][book] = {}

        if chapter not in bible_dict[translation][book]:
            bible_dict[translation][book][chapter] = {}

        bible_dict[translation][book][chapter][verse] = text

# List of all 66 books in canonical (Protestant) order
books_in_order = [
    "Genesis", "Exodus", "Leviticus", "Numbers", "Deuteronomy",
    "Joshua", "Judges", "Ruth", "1 Samuel", "2 Samuel",
    "1 Kings", "2 Kings", "1 Chronicles", "2 Chronicles", "Ezra",
    "Nehemiah", "Esther", "Job", "Psalms", "Proverbs",
    "Ecclesiastes", "Song of Solomon", "Isaiah", "Jeremiah", "Lamentations",
    "Ezekiel", "Daniel", "Hosea", "Joel", "Amos",
    "Obadiah", "Jonah", "Micah", "Nahum", "Habakkuk",
    "Zephaniah", "Haggai", "Zechariah", "Malachi",
    "Matthew", "Mark", "Luke", "John", "Acts",
    "Romans", "1 Corinthians", "2 Corinthians", "Galatians", "Ephesians",
    "Philippians", "Colossians", "1 Thessalonians", "2 Thessalonians", "1 Timothy",
    "2 Timothy", "Titus", "Philemon", "Hebrews", "James",
    "1 Peter", "2 Peter", "1 John", "2 John", "3 John",
    "Jude", "Revelation"
]

# Create a dictionary to map each book to its order number
book_order = {book_name: idx for idx, book_name in enumerate(books_in_order, start=1)}

ot_books = [
    "Genesis", "Exodus", "Leviticus", "Numbers", "Deuteronomy",
    "Joshua", "Judges", "Ruth", "1 Samuel", "2 Samuel",
    "1 Kings", "2 Kings", "1 Chronicles", "2 Chronicles", "Ezra",
    "Nehemiah", "Esther", "Job", "Psalms", "Proverbs",
    "Ecclesiastes", "Song of Solomon", "Isaiah", "Jeremiah", "Lamentations",
    "Ezekiel", "Daniel", "Hosea", "Joel", "Amos",
    "Obadiah", "Jonah", "Micah", "Nahum", "Habakkuk",
    "Zephaniah", "Haggai", "Zechariah", "Malachi"
]

nt_books = [
    "Matthew", "Mark", "Luke", "John", "Acts",
    "Romans", "1 Corinthians", "2 Corinthians", "Galatians", "Ephesians",
    "Philippians", "Colossians", "1 Thessalonians", "2 Thessalonians", "1 Timothy",
    "2 Timothy", "Titus", "Philemon", "Hebrews", "James", "1 Peter", "2 Peter",
    "1 John", "2 John", "3 John", "Jude", "Revelation"
]

key_books = ["Genesis", "Psalms", "Isaiah", "John", "Romans", "Revelation"]

key_words = ["god", "sin", "mercy", "grace", "forgiveness", "salvation", "jesus", "wrath", "hope", "anger", "heaven", "death"]

translation_years = {
    "t_kjv": 1611,
    "t_asv": 1901,
    "t_bbe": 1965,
    "t_ylt": 1862,
    "t_wbt": 1833,
    "t_web": 1994
}

!pip install nltk

import nltk
nltk.download('vader_lexicon')
nltk.download('punkt_tab')

archaic_to_modern = {
    "thou": "you",
    "thee": "you",
    "thy": "your",
    "thine": "yours",
    "ye": "you",
    "mine": "my", # approximate; sometimes "mine" should remain "mine"
    "art": "are",
    "shalt": "shall",
    "wilt": "will",
    "hath": "has",
    "doth": "does",
    "doeth": "does",
    "doest": "do",
    "cometh": "comes",
    "goeth": "goes",
    "saith": "says",
    "sayeth": "says",
    "taketh": "takes",
    "takest": "take",
    "knoweth": "knows",
    "knowest": "know",
    "knewest": "knew",
    "didst": "did",
    "hadst": "had",
    "spake": "spoke",
    "spakest": "spoke",
    "spaketh": "spoke",
    "wast": "were",
    "wert": "were",
    "mayest": "may",
    "mightest": "might",
    "wouldest": "would",
    "shouldest": "should",
    "couldest": "could",
    "hither": "here",
    "thither": "there",
    "whither": "where",
    "whence": "from where",
    "wherefore": "why",
    "heretofore": "before now",
    "lest": "or else", # approximate
    "lo": "look",
    "behold": "look", # approximate
    "nigh": "near",
    "unto": "to", # or “toward,” depending on context
    "whosoever": "whoever",
    "whoso": "whoever",
    "wherein": "in which",
    "whereon": "on which",
    "thereof": "of it",
    "verily": "truly",
    "exhort": "encourage", # approximate; “exhort” is somewhat archaic or formal
    "recompense": "repay" # often used in older translations
}

def replace_archaic_words(text, replacements=archaic_to_modern):
    tokens = re.split(r"(\s+)", text)  # keep whitespace as separate tokens
    new_tokens = []
    for tok in tokens:
        lower_tok = tok.lower()
        if lower_tok in replacements:
            new_tokens.append(replacements[lower_tok])
        else:
            new_tokens.append(tok)
    return "".join(new_tokens)

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_book_text(bible_dict, translation, book):
    """
    Gathers and preprocesses all verses from a single book in a given translation.
    Returns a list of verse texts (preprocessed).
    """
    verses_preprocessed = []

    if translation not in bible_dict:
        raise ValueError(f"Translation '{translation}' not found in bible_dict keys: {bible_dict.keys()}")
    if book not in bible_dict[translation]:
        raise ValueError(f"Book '{book}' not found in translation '{translation}'")

    book_data = bible_dict[translation][book] # should be dict of {chapter: {verse: text}}

    for chapter, verses_dict in book_data.items():
        for verse_num, verse_text in verses_dict.items():

            text = verse_text.lower()
            text = re.sub(r"[^\w\s]", "", text)
            text = replace_archaic_words(text)
            text = text.strip()
            words = text.split()
            words = [word for word in words if word not in stop_words]
            text = " ".join(words)

            verses_preprocessed.append(text)

    return verses_preprocessed

from nltk.sentiment import SentimentIntensityAnalyzer

def analyze_book_sentiment(preprocessed_verses):
    """
    Given a list of preprocessed verse texts, run them through VADER
    and return a list of sentiment scores (one dict per verse).
    """
    sia = SentimentIntensityAnalyzer()
    sentiments = []

    for verse_text in preprocessed_verses:
        scores = sia.polarity_scores(verse_text)
        # scores is dict like {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}
        sentiments.append(scores)

    return sentiments

def get_average_sentiment_for_book(bible_dict, translation, book):
    """
    Preprocess a single book for a single translation, run VADER,
    and return the average compound sentiment score.
    """
    verses = preprocess_book_text(bible_dict, translation, book)

    verse_scores = analyze_book_sentiment(verses)

    if len(verse_scores) == 0:
        return None

    avg_compound = sum(s["compound"] for s in verse_scores) / len(verse_scores)
    return avg_compound

import matplotlib.pyplot as plt

def plot_book_sentiment_by_year(bible_dict, book, translation_years):
    """
    For a given book (e.g. 'Genesis'), compute the average VADER compound
    sentiment across all translations that have that book. Then plot
    sentiment vs. year of translation publication.
    """
    years = []
    scores = []
    labels = []

    for translation, year in translation_years.items():
        # for safety
        if translation not in bible_dict:
            continue
        if book_order[book] not in bible_dict[translation]:
            continue

        avg_score = get_average_sentiment_for_book(bible_dict, translation, book_order[book])
        if avg_score is not None:
            years.append(year)
            scores.append(avg_score)
            labels.append(translation)

    # More safety
    if not years:
        print(f"No sentiment data found for book='{book}' in the provided translations.")
        return

    # scatter plot
    plt.figure(figsize=(8, 6))
    plt.scatter(years, scores, color='blue')

    # label each point with the translation
    for x, y, label in zip(years, scores, labels):
        plt.text(x, y, label, fontsize=9, ha='left', va='bottom')

    plt.xlabel("Year of Translation")
    plt.ylabel("Average Compound Sentiment")
    plt.title(f"Sentiment of '{book}' by Translation Year")
    plt.grid(True)
    plt.xlim([min(years) - 10, max(years) + 10])

    plt.show()

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('sentiwordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')

from nltk.corpus import wordnet

def nltk_pos_to_wordnet_pos(nltk_pos):
    """
    Convert NLTK's part-of-speech tags to WordNet's pos tags:
      - 'n' for nouns
      - 'v' for verbs
      - 'a' for adjectives
      - 'r' for adverbs
    Returns 'n', 'v', 'a', 'r' or None if not recognized.
    """
    if nltk_pos.startswith('J'):
        return wordnet.ADJ
    elif nltk_pos.startswith('V'):
        return wordnet.VERB
    elif nltk_pos.startswith('N'):
        return wordnet.NOUN
    elif nltk_pos.startswith('R'):
        return wordnet.ADV
    else:
        return None

import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import sentiwordnet as swn
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def get_sentiwordnet_sentiment(verse_text):
    """
    Given a verse, compute an overall sentiment score using SentiWordNet.
    compute (pos_score - neg_score) across all tokens.

    Returns a float representing the sentimen
    """
    # little cleanup
    verse_text = verse_text.lower()
    verse_text = re.sub(r'[^\w\s]', '', verse_text)

    tokens = word_tokenize(verse_text)
    tagged_tokens = pos_tag(tokens)

    if not tokens:
        return 0.0

    total_score = 0.0
    count = 0

    for token, tag in tagged_tokens:
        wn_tag = nltk_pos_to_wordnet_pos(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(token, pos=wn_tag)

        synsets = list(wordnet.synsets(lemma, pos=wn_tag))
        if len(synsets) == 0:
            continue

        synset = synsets[0]

        swn_synset = swn.senti_synset(synset.name())
        pos_score = swn_synset.pos_score()
        neg_score = swn_synset.neg_score()

        net = pos_score - neg_score
        total_score += net
        count += 1

    if count == 0:
        return 0.0

    return total_score / count

def get_book_sentiwordnet_score(bible_dict, translation, book):
    """
    Compute an average net SentiWordNet sentiment score
    (pos_score - neg_score) across all verses of 'book' in 'translation'
    """
    if translation not in bible_dict:
        return None
    if book not in bible_dict[translation]:
        return None

    book_data = bible_dict[translation][book]  # {chapter: {verse: text}}
    scores = []
    for chapter, verses_dict in book_data.items():
        for verse_num, verse_text in verses_dict.items():
            verse_score = get_sentiwordnet_sentiment(verse_text)
            scores.append(verse_score)

    if len(scores) == 0:
        return None
    return sum(scores) / len(scores)

import matplotlib.pyplot as plt
nltk.download('averaged_perceptron_tagger_eng')

def plot_book_sentiment_by_year_swn(bible_dict, book, translation_years):
    """
    For a given book compute the average SentiWordNet-based
    sentiment across all translations that have that book
    Then plot sentiment vs. year of translation

    bible_dict: nested dict {translation -> book -> chapter -> verse -> text}
    translation_years: dict {translation_key: publication_year}
    """
    years = []
    scores = []
    labels = []

    for translation, year in translation_years.items():
        score = get_book_sentiwordnet_score(bible_dict, translation, book_order[book])
        if score is not None:
            years.append(year)
            scores.append(score)
            labels.append(translation)

    if not years:
        print(f"No data found for book='{book}' across these translations.")
        return

    # Plot
    plt.figure(figsize=(8, 6))
    plt.scatter(years, scores, color='green')

    # Label points
    for x, y, label in zip(years, scores, labels):
        plt.text(x, y, label, fontsize=9, ha='left', va='bottom')

    plt.xlabel("Year of Translation")
    plt.ylabel("Average SentiWordNet Score (pos - neg)")
    plt.title(f"'{book}' SentiWordNet Sentiment by Translation Year")
    plt.grid(True)

    # adjust x-axis range
    plt.xlim([min(years) - 10, max(years) + 10])

    plt.show()

import matplotlib.pyplot as plt

def plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices,
    sentiment_func,
    book_order,
    translation_years,
    plot_title="Aggregate Sentiment for Selected Books"
):
    """
    Aggregates sentiment across a set of books (given by their order indices),
    for each translation in 'translation_years'. Then plots those aggregated
    scores against the translation's publication year.
    """

    years = []
    aggregated_scores = []
    labels = []

    for translation, year in translation_years.items():
        book_scores = []
        for book_num in book_indices:
            if translation in bible_dict and book_num in bible_dict[translation]:
                score = sentiment_func(bible_dict, translation, book_num)
                if score is not None:
                    book_scores.append(score)

        if len(book_scores) > 0:
            avg_score = sum(book_scores) / len(book_scores)
            years.append(year)
            aggregated_scores.append(avg_score)
            labels.append(translation)

    if not years:
        print("No data found for the given books and translations.")
        return

    plt.figure(figsize=(8, 6))
    plt.scatter(years, aggregated_scores, color='red')

    # label each point
    for x, y, lbl in zip(years, aggregated_scores, labels):
        plt.text(x, y, lbl, fontsize=9, ha='left', va='bottom')

    plt.xlabel("Year of Translation")
    plt.ylabel("Aggregate Sentiment Score")
    plt.title(plot_title)
    plt.grid(True)

    # for padding
    xmin, xmax = min(years), max(years)
    plt.xlim([xmin - 10, xmax + 10])

    plt.show()

ot_indices = list(range(1, 40))
nt_indices = list(range(41, 66))

plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=ot_indices,
    sentiment_func=get_book_sentiwordnet_score,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average SWN Sentiment for Old Testament"
)

plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=nt_indices,
    sentiment_func=get_book_sentiwordnet_score,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average SWN Sentiment for New Testament"
)

plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=ot_indices,
    sentiment_func=get_average_sentiment_for_book,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average VADER Sentiment for Old Testament"
)

plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=nt_indices,
    sentiment_func=get_average_sentiment_for_book,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average VADER Sentiment for New Testament"
)

plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=[book_order[book] for book in key_books],
    sentiment_func=get_book_sentiwordnet_score,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average SWN Sentiment for Key Theological Books"
)


plot_aggregate_sentiment_across_books(
    bible_dict,
    book_indices=[book_order[book] for book in key_books],
    sentiment_func=get_average_sentiment_for_book,
    book_order=book_order,
    translation_years=translation_years,
    plot_title="Average VADER Sentiment for Key Theological Books"
)

import seaborn as sns
sid = SentimentIntensityAnalyzer()

def compute_sentiment_for_keywords(bible_dict, translation_years, key_words):
    """
    For each translation, each book, preprocess the book text,
    then compute an average sentiment score (VADER) for sentences containing each keyword.

    Returns a list of dicts that can be converted into a DataFrame.
    """
    sentiment_data = []

    for translation, books_dict in bible_dict.items():
        if translation not in translation_years:
            continue

        year = translation_years[translation]

        for book in books_dict.keys():
            preprocessed_verses = preprocess_book_text(bible_dict, translation, book)
            book_text = " ".join(preprocessed_verses)
            sentences = nltk.sent_tokenize(book_text)

            for kw in key_words:
                relevant_sentences = [s for s in sentences if kw.lower() in s.lower()]

                if relevant_sentences:
                    scores = [sid.polarity_scores(s)["compound"] for s in relevant_sentences]
                    avg_score = sum(scores) / len(scores)

                    # store res
                    sentiment_data.append({
                        "translation": translation,
                        "year": year,
                        "book": book,
                        "keyword": kw,
                        "avg_sentiment": avg_score
                    })

    return sentiment_data

def visualize_sentiment_drift(sentiment_data, key_words):
    """
    Creates line plots (one per keyword) of average sentiment by year,
    separated by translation.
    """
    df = pd.DataFrame(sentiment_data)

    df_grouped = (
        df.groupby(["translation", "year", "keyword"], as_index=False)["avg_sentiment"]
          .mean()
    )

    # Plot one figure per keyword
    for kw in key_words:
        subset = df_grouped[df_grouped["keyword"] == kw].copy()
        subset.sort_values("year", inplace=True)

        plt.figure(figsize=(8, 5))
        sns.lineplot(data=subset, x="year", y="avg_sentiment", marker="o", hue="translation")
        plt.title(f"Sentiment Drift for '{kw}' across Translations")
        plt.xlabel("Translation Year")
        plt.ylabel("Average VADER Sentiment (Compound)")
        plt.legend(title="Translation")
        plt.grid(True)
        plt.show()

key_word_data = compute_sentiment_for_keywords(bible_dict, translation_years, key_words)

visualize_sentiment_drift(key_word_data, key_words)

from collections import Counter

def get_top_cooccurring_words(bible_dict,
                              key_words,
                              window_size=5,
                              top_n=10):
    """
    Returns a dict mapping each keyword -> list of (word, count) for the top_n words.
    """
    co_occurrences = {kw: Counter() for kw in key_words}

    for translation, books_dict in bible_dict.items():
        for book in books_dict.keys():
            preprocessed_verses = preprocess_book_text(bible_dict, translation, book)

            tokens = []
            for verse_text in preprocessed_verses:
                tokens.extend(verse_text.split())

            for i, token in enumerate(tokens):
                if token in co_occurrences.keys():
                    kw = token  # which keyword was matched
                    start_idx = max(0, i - window_size)
                    end_idx   = min(len(tokens), i + window_size + 1)

                    for j in range(start_idx, end_idx):
                        if j != i:  # don't count the keyword itself
                            co_occurrences[kw][tokens[j]] += 1

    top_words = {}
    for kw, counter in co_occurrences.items():
        top_words[kw] = counter.most_common(top_n)

    return top_words

top_cooccur = get_top_cooccurring_words(
    bible_dict=bible_dict,
    key_words=key_words,
    window_size=3,
    top_n=10
)

for kw, words in top_cooccur.items():
    print(f"Keyword '{kw}' top co-occurring words:")
    for w, count in words:
        print(f"  {w}: {count}")
    print()

!pip install gensim
import re
import nltk
import gensim
from gensim.models import Word2Vec
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def get_translation_sentences(bible_dict, translation):
    """
    Gather all text for a single translation, tokenize into sentences,
    then return a list-of-lists of word tokens for each sentence.
    """
    if translation not in bible_dict:
        raise ValueError(f"Translation '{translation}' not found.")

    all_text = []

    for book, chapters_dict in bible_dict[translation].items():
        for chapter, verses_dict in chapters_dict.items():
            for verse_num, verse_text in verses_dict.items():
                all_text.append(verse_text)

    concatenated_text = " ".join(all_text)

    raw_sentences = nltk.sent_tokenize(concatenated_text)

    processed_sentences = []
    for sent in raw_sentences:
        sent = sent.lower()
        sent = re.sub(r"[^\w\s]", "", sent)
        words = sent.split()
        words = [w for w in words if w not in stop_words]
        if words:
            processed_sentences.append(words)

    return processed_sentences

def train_models_by_translation(bible_dict,
                                vector_size=100,
                                min_count=2,
                                window=5,
                                workers=4):
    """
    For each translation in bible_dict, train a Word2Vec model on that translation's text.
    Returns a dict: {translation: trained Word2Vec model}
    """
    models = {}

    for translation in bible_dict.keys():
        sentences = get_translation_sentences(bible_dict, translation)

        model = Word2Vec(
            sentences=sentences,
            vector_size=vector_size,
            window=window,
            min_count=min_count,
            workers=workers
        )

        models[translation] = model

    return models

def get_closest_words_by_translation(models, keywords, topn=10):
    """
    For each translation, for each keyword, get the topn most similar words
    in that translation's embedding space
    """
    results = {}

    for translation, w2v_model in models.items():
        translation_dict = {}

        for kw in keywords:
            if kw in w2v_model.wv.key_to_index:
                similar = w2v_model.wv.most_similar(kw, topn=topn)
                translation_dict[kw] = similar
            else:
                translation_dict[kw] = []

        results[translation] = translation_dict

    return results

models_by_translation = train_models_by_translation(bible_dict, vector_size=100, min_count=1)

closest_words = get_closest_words_by_translation(models_by_translation, key_words, topn=5)

for translation, kw_dict in closest_words.items():
    print(f"--- {translation} ---")
    for kw, word_list in kw_dict.items():
        print(f"Keyword '{kw}':")
        if word_list:
            for w, sim in word_list:
                print(f"    {w} ({sim:.3f})")
        else:
            print("(keyword not found in vocabulary or too rare)")
    print()

import networkx as nx
import matplotlib.pyplot as plt

def visualize_translation_keywords(closest_words):
    """
    Creates a separate network graph for each translation
    """
    for translation, kw_dict in closest_words.items():
        G = nx.Graph()

        keyword_nodes = []
        word_nodes = []

        for kw, word_list in kw_dict.items():
            # Add the keyword node if not present
            if kw not in G:
                G.add_node(kw, node_type="keyword")
                keyword_nodes.append(kw)

            # For each (word, similarity) pair, connect to the keyword
            for word, sim in word_list:
                if word not in G:
                    G.add_node(word, node_type="word")
                    word_nodes.append(word)

                G.add_edge(kw, word, weight=sim)

        pos = nx.spring_layout(G, seed=42)

        plt.figure(figsize=(8, 6))

        nx.draw_networkx_nodes(
            G, pos,
            nodelist=keyword_nodes,
            node_color="lightgreen",
            node_shape="o",
            node_size=800,
            label="Keywords"
        )

        nx.draw_networkx_nodes(
            G, pos,
            nodelist=word_nodes,
            node_color="lightblue",
            node_shape="s",
            node_size=600,
            label="Similar Words"
        )

        nx.draw_networkx_edges(G, pos, alpha=0.5)

        edges = G.edges(data=True)
        nx.draw_networkx_edges(
            G, pos,
            edgelist=edges,
            width=[data['weight'] * 2 for (_,_,data) in edges],
            alpha=0.5
        )

        # Draw labels
        labels = {node: node for node in G.nodes()}
        nx.draw_networkx_labels(G, pos, labels=labels, font_size=10)

        plt.title(f"Top Similar Words per Keyword - {translation}")
        plt.legend()
        plt.axis("off")
        plt.show()

visualize_translation_keywords(closest_words)

"""# Part Two: Models"""

!pip install transformers torch

from transformers import pipeline

# Create a sentiment pipeline (DistilBERT)
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

def get_sentiment_scores_in_chunks(sentiment_pipeline, verses, chunk_size=10):
    total_verses = len(verses)
    all_results = []

    for start_idx in range(0, total_verses, chunk_size):
        end_idx = start_idx + chunk_size
        chunk = verses[start_idx:end_idx]

        chunk_results = sentiment_pipeline(chunk, truncation=True)

        for verse, pred in zip(chunk, chunk_results):
            all_results.append({
                "verse": verse,
                "label": pred["label"],
                "score": pred["score"]
            })

        print(f"Processed {min(end_idx, total_verses)} out of {total_verses} verses...")

    return all_results

def aggregate_verse_sentiments(verse_results):
    """
    Given a list of verse-level sentiment dictionaries compute 'overall_score'.
    If label == POSITIVE, add score
    If label == NEGATIVE, subtract score
    Divide by number of verses
    Returns a float in range([-1, +1])
    """
    if not verse_results:
        return 0.0  # or None, if no data

    total = 0.0
    for result in verse_results:
        label = result["label"].upper()
        score = result["score"]
        if label == "POSITIVE":
            total += score
        elif label == "NEGATIVE":
            total -= score
        else:
            # If there's a label that's neither + or -,
            pass

    return total / len(verse_results)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def plot_aggregate_sentiment_over_years(
    bible_dict,
    books,
    sentiment_pipeline,
    translation_years,
    plot_title="Aggregate Sentiment Over Years",
    book_order=None,
    save_path="/content/drive/MyDrive/aggregate_sentiment_results.csv"
):
    """
    For each translation in translation_years run them through the given
    sentiment_pipeline, aggregate the sentiment,
    and plot the result vs. the translation's publication year.

    also create a pandas DataFrame of the results, saves it to a CSV,
    and adds a line of best fit (with slope calculation) to the scatter plot.
    """

    if isinstance(books, str):
        books = [books]

    years = []
    aggregated_scores = []
    labels = []

    for translation, pub_year in translation_years.items():
        preprocessed_verses = []
        for book in books:
            book_index = book_order[book]
            preprocessed_verses.extend(preprocess_book_text(bible_dict, translation, book_index))

        book_level_results = get_sentiment_scores_in_chunks(
            sentiment_pipeline,
            preprocessed_verses,
            chunk_size=10
        )

        agg_score = aggregate_verse_sentiments(book_level_results)

        years.append(pub_year)
        aggregated_scores.append(agg_score)
        labels.append(translation)

    plt.figure(figsize=(8, 6))
    plt.scatter(years, aggregated_scores, color="blue", label="Translations")

    z = np.polyfit(years, aggregated_scores, 1)
    slope, intercept = z
    p = np.poly1d(z)

    # Generate x values for the line of best fit
    x_range = np.linspace(min(years), max(years), 100)
    y_fit = p(x_range)
    plt.plot(x_range, y_fit, 'r--', label=f"Best Fit (slope={slope:.3f})")

    # Label each point with the translation key
    for x, y, lbl in zip(years, aggregated_scores, labels):
        plt.text(x, y, lbl, fontsize=9, ha="left", va="bottom")

    plt.title(plot_title)
    plt.xlabel("Year of Translation")
    plt.ylabel("Aggregate Sentiment (DistilBERT)")
    plt.grid(True)

    xmin, xmax = min(years), max(years)
    plt.xlim(xmin - 10, xmax + 10)

    plt.legend()
    plt.show()

    print(f"Line of best fit: slope = {slope:.3f}, intercept = {intercept:.3f}")

    results_df = pd.DataFrame({
        "Translation": labels,
        "Year": years,
        "AggregateScore": aggregated_scores
    })

    # Save DataFrame to CSV
    results_df.to_csv(save_path, index=False)
    print(f"Results saved to {save_path}")

    return results_df

plot_aggregate_sentiment_over_years(
    bible_dict,
    books=["Matthew", "Mark", "Luke", "John"],
    sentiment_pipeline=sentiment_analyzer,
    translation_years=translation_years,
    plot_title="4 Gospels Aggregate Sentiment by Translation Year",
    book_order=book_order,
    save_path="/content/drive/MyDrive/bible_project/gospels_books_sentiment_results.csv"
)

plot_aggregate_sentiment_over_years(
    bible_dict,
    books=key_books,
    sentiment_pipeline=sentiment_analyzer,
    translation_years=translation_years,
    plot_title="Key Books Aggregate Sentiment by Translation Year",
    book_order=book_order,
    save_path="/content/drive/MyDrive/bible_project/key_books_sentiment_results.csv"
)

plot_aggregate_sentiment_over_years(
       bible_dict,
       books=ot_books,
       sentiment_pipeline=sentiment_analyzer,
       translation_years=translation_years,
       plot_title="Old Testament Aggregate Sentiment by Translation Year",
       book_order=book_order,
       save_path="/content/drive/MyDrive/bible_project/ot_sentiment_results.csv"
   )

plot_aggregate_sentiment_over_years(
    bible_dict,
    books=nt_books,
    sentiment_pipeline=sentiment_analyzer,
    translation_years=translation_years,
    plot_title="New Testament Aggregate Sentiment by Translation Year",
    book_order=book_order,
    save_path="/content/drive/MyDrive/bible_project/nt_sentiment_results.csv"
)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from transformers import DistilBertTokenizer, DistilBertModel
import torch

# Load DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertModel.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

def get_embedding(text):
    """Computes DistilBERT embeddings for a given text."""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def compute_embeddings(target_keyword, synonyms, bible_dict, translation_years):
    """Computes embeddings for the target keyword and its synonyms across translations."""
    embedding_data = []

    for translation, books in bible_dict.items():
        year = translation_years.get(translation, None)
        verses = " ".join([verse_text for chapters in books.values() for verses in chapters.values() for verse_text in verses.values()])
        embedding = get_embedding(verses.replace(target_keyword, "[MASK]"))
        embedding_data.append({"translation": translation, "year": year, "keyword": target_keyword, "embedding": embedding})

    df = pd.DataFrame(embedding_data)

    # Compute embeddings for synonyms
    neighbor_embeddings = np.array([get_embedding(word) for word in synonyms])
    neighbor_df = pd.DataFrame({"word": synonyms, "embedding": list(neighbor_embeddings)})

    return df, neighbor_df

def reduce_dimensionality(df, neighbor_df):
    """Reduces embedding dimensionality using t-SNE."""
    tsne = TSNE(n_components=2, random_state=42, perplexity=3)

    # process translation embeddings
    embedding_matrix = np.vstack(df["embedding"].values)
    embedding_2d = tsne.fit_transform(embedding_matrix)
    df["x"], df["y"] = embedding_2d[:, 0], embedding_2d[:, 1]

    # process synonym embeddings
    neighbor_matrix = np.vstack(neighbor_df["embedding"].values)
    neighbor_2d = tsne.fit_transform(neighbor_matrix)
    neighbor_df["x"], neighbor_df["y"] = neighbor_2d[:, 0], neighbor_2d[:, 1]

    return df, neighbor_df

def plot_results(df, neighbor_df, target_keyword):
    """Plots the results using seaborn."""
    plt.figure(figsize=(12, 8))

    # keyword drift points
    sns.scatterplot(data=df, x="x", y="y", hue="translation", palette="deep", s=100)

    # similar words
    sns.scatterplot(data=neighbor_df, x="x", y="y", color="gray", marker="X", s=100, label="Neighbor Words")

    # line connecting the translations in chronological order
    df = df.sort_values(by="year")
    plt.plot(df["x"], df["y"], linestyle="-", color="black", alpha=0.5, marker="o")

    # anotate points
    for i in range(len(df)):
        plt.text(df["x"].iloc[i], df["y"].iloc[i], f'{df["translation"].iloc[i]} ({df["year"].iloc[i]})', fontsize=9)

    for i in range(len(neighbor_df)):
        plt.text(neighbor_df["x"].iloc[i], neighbor_df["y"].iloc[i], neighbor_df["word"].iloc[i], fontsize=9, color="black")

    plt.title(f"Semantic Drift of '{target_keyword}' Across Bible Translations (t-SNE)")
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.legend(title="Category")
    plt.grid(True)
    plt.show()

target_keyword = "grace"
synonyms = [
    # Synonyms
    "mercy", "kindness", "favor", "blessing", "benevolence",

    # Theological & Religious Terms
    "salvation", "redemption", "atonement", "justification", "sanctification",

    # Opposites & Antonyms
    "wrath", "judgment", "condemnation",

    # Biblical Figures & Contexts
    "god", "christ", "jesus", "holy",

    # Attributes of Grace
    "beauty", "elegance", "poise",

    # Consequences of Grace
    "peace", "joy", "love",
]

df, neighbor_df = compute_embeddings(target_keyword, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, target_keyword)

target_keyword = "god"
synonyms = [
    # Synonyms & Divine Titles
    "lord", "creator", "almighty", "father", "deity",
    "yahweh", "jehovah", "elohim", "adonai",

    # Theological & Religious Terms
    "omnipotence", "omniscience", "omnipresence",

    # Opposites & Contrasts
    "devil", "satan", "evil"

    # Biblical Figures
    "jesus", "abraham",

    # Attributes of God
    "love", "mercy", "justice",

    # Consequences of Belief in God
    "faith", "hope", "peace",
]

df, neighbor_df = compute_embeddings(target_keyword, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, target_keyword)

target_keyword = "jesus"
synonyms = word_list = [
    # Titles & Synonyms
    "christ", "messiah", "savior", "redeemer",

    # Theological & Religious Terms
    "salvation", "atonement", "grace",

    # Opposites & Contrasts
    "satan", "evil", "sin",

    # Biblical Figures & Contexts
    "mary", "joseph", "john", "peter", "paul",

    # Attributes of Jesus
    "love", "mercy", "compassion",

    # Consequences of Belief in Jesus
    "faith", "eternal", "grace",

    # Historical & Cultural Contexts
    "bethlehem", "nazareth",

]

df, neighbor_df = compute_embeddings(target_keyword, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, target_keyword)

target_keyword = "sin"
synonyms = [
    # Synonyms
    "transgression", "iniquity", "wickedness",

    # Religious/Theological Terms
    "forgiveness", "atonement", "repentance",

    # Opposites & Antonyms
    "virtue", "righteousness", "innocence",

    # Biblical Figures
    "adam", "eve", "serpent", "satan",

    # Consequences of Sin
    "punishment", "wrath", "guilt", "shame"
]
df, neighbor_df = compute_embeddings(target_keyword, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, target_keyword)

key_words = ["god", "sin", "mercy", "grace", "forgiveness", "salvation", "jesus", "wrath", "hope", "anger", "heaven", "death"]

key_word = "death"
synonyms = [
    # Synonyms & Related Terms
    "dying", "demise", "passing",

    # Theological & Religious Terms
    "afterlife", "resurrection",

    # Opposites & Contrasts
    "life", "birth", "rebirth",

    # Biblical Figures & Contexts
    "jesus", "god", "lazarus",

    # Causes of Death
    "disease", "injury", "suffering", "war",
]

df, neighbor_df = compute_embeddings(key_word, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, key_word)

key_word = "salvation"
synonyms = word_list = [
    # Synonyms & Related Terms
    "redemption", "deliverance", "atonement", "saving", "conversion", "repentance",

    # Theological & Religious Terms
    "baptism", "heaven", "resurrection", "sanctification", "sanctuary", "church",

    # Opposites & Contrasts
    "damnation", "condemnation", "sin",

    # Biblical Figures & Contexts
    "jesus", "christ", "god",

    # Attributes of Salvation
    "peace", "hope", "love",

    # Consequences of Salvation
    "eternity", "afterlife",
]
df, neighbor_df = compute_embeddings(key_word, synonyms, bible_dict, translation_years)
df, neighbor_df = reduce_dimensionality(df, neighbor_df)
plot_results(df, neighbor_df, key_word)

"""# Zero-Shot vs. Few-Shot Sentiment Analysis (Hugging Face)"""

!pip install transformers torch pandas tqdm

from transformers import pipeline

# Load RoBERTa Large MNLI for Zero-Shot Classification
classifier = pipeline("zero-shot-classification", model="roberta-large-mnli")

# Define sentiment categories
labels = ["positive", "neutral", "negative"]

few_shot_examples = """
Analyze the sentiment of the following Bible verse. Use either "positive", "neutral", or "negative".

Examples:

1 "The Lord is my shepherd, I shall not want." → Positive
2 "Blessed are the meek, for they shall inherit the earth." → Positive
3 "Fear not, for I am with you; be not dismayed, for I am your God." → Positive
4 "The Lord is my light and my salvation—whom shall I fear?" → Positive
5 "For God so loved the world, that he gave his only begotten Son." → Positive
6 "The joy of the Lord is your strength." → Positive
7 "I can do all things through Christ who strengthens me." → Positive
8 "Be strong and courageous; do not be afraid, for the Lord your God goes with you." → Positive

9 "My God, my God, why have you forsaken me?" → Negative
10 "Woe to the rebellious children, saith the Lord, that take counsel, but not of me." → Negative
11 "The wages of sin is death, but the gift of God is eternal life through Jesus Christ our Lord." → Nuetral
12 "And Judas went and hanged himself." → Negative
13 "For many are called, but few are chosen." → Negative
14 "You are of your father the devil, and your will is to do your father’s desires." → Negative
15 "Cursed is the man who trusts in man and makes flesh his strength." → Negative
16 "The heart is deceitful above all things, and desperately wicked: who can know it?" → Negative

17 "Now these are the names of the sons of Israel, who came into Egypt with Jacob." → Neutral
18 "This is the book of the generations of Adam." → Neutral
19 "In the beginning, God created the heavens and the earth." → Neutral
20 "And Noah was six hundred years old when the flood of waters was upon the earth." → Neutral
21 "Jesus went up to Jerusalem for the Feast of the Passover." → Neutral
22 "Paul, an apostle of Jesus Christ, by the will of God, to the saints in Ephesus." → Neutral
23 "And the Lord spoke to Moses, saying, 'Command the Israelites to bring you clear oil for the lampstand.'" → Neutral

Now analyze this verse:
"{verse}"
Sentiment:
"""


# Function to compare Few-Shot vs Zero-Shot
def compare_fewshot_vs_zeroshot(verse):
    """
    Compares Few-Shot vs Zero-Shot sentiment analysis on a given Bible verse.
    """

    # Zero-Shot Analysis (No Examples)
    zero_shot_result = classifier(verse, candidate_labels=labels)

    # Few-Shot Analysis (Providing Examples in the Prompt)
    few_shot_prompt = few_shot_examples.format(verse=verse)
    few_shot_result = classifier(few_shot_prompt, candidate_labels=labels)

    # Print Results
    print("\nZero-Shot Sentiment Analysis")
    print(f"Verse: \"{verse}\"")
    print(f"Prediction: {zero_shot_result['labels'][0]} ({zero_shot_result['scores'][0]:.2f})")

    print("\nFew-Shot Sentiment Analysis")
    print(f"Verse: \"{verse}\"")
    print(f"Prediction: {few_shot_result['labels'][0]} ({few_shot_result['scores'][0]:.2f})")

    return {"zero_shot": zero_shot_result, "few_shot": few_shot_result}

verse = "For God so loved the world, that he gave his only begotten Son."
comparison_result = compare_fewshot_vs_zeroshot(verse)

verse = "Jesus wept"
comparison_result = compare_fewshot_vs_zeroshot(verse)

verse = "For the Son of man is come to seek and to save that which was lost."
comparison_result = compare_fewshot_vs_zeroshot(verse)

"""Zero-Shot vs. Few-Shot (Together AI)"""

pip install together

import together

# Set API key
together.api_key = "01080322105ed03126e4e1b2dce2a56a1a68fb95e3dab6b5f0cf8813733f17a0"

def zero_shot_sentiment(text):
    response = together.Complete.create(
        model="mistralai/Mistral-7B-Instruct-v0.2",
        prompt=f"Classify the sentiment of the following text. Limit your response to just Positive, Neutral, or Negative: {text}",
        max_tokens=10
    )

    return response["choices"][0]["text"].strip()

def few_shot_sentiment(text):
    prompt = """
    Classify the sentiment of the following text. Limit your response to just Positive, Neutral, or Negative

    Examples:

    1 "The Lord is my shepherd, I shall not want." → Positive
    2 "Blessed are the meek, for they shall inherit the earth." → Positive
    3 "Fear not, for I am with you; be not dismayed, for I am your God." → Positive
    4 "The Lord is my light and my salvation—whom shall I fear?" → Positive
    5 "For God so loved the world, that he gave his only begotten Son." → Positive
    6 "The joy of the Lord is your strength." → Positive
    7 "I can do all things through Christ who strengthens me." → Positive
    8 "Be strong and courageous; do not be afraid, for the Lord your God goes with you." → Positive
    9 "My God, my God, why have you forsaken me?" → Negative
    10 "Woe to the rebellious children, saith the Lord, that take counsel, but not of me." → Negative
    11 "The wages of sin is death, but the gift of God is eternal life through Jesus Christ our Lord." → Nuetral
    12 "And Judas went and hanged himself." → Negative
    13 "For many are called, but few are chosen." → Negative
    14 "You are of your father the devil, and your will is to do your father’s desires." → Negative
    15 "Cursed is the man who trusts in man and makes flesh his strength." → Negative
    16 "The heart is deceitful above all things, and desperately wicked: who can know it?" → Negative
    17 "Now these are the names of the sons of Israel, who came into Egypt with Jacob." → Neutral
    18 "This is the book of the generations of Adam." → Neutral
    19 "In the beginning, God created the heavens and the earth." → Neutral
    20 "And Noah was six hundred years old when the flood of waters was upon the earth." → Neutral
    21 "Jesus went up to Jerusalem for the Feast of the Passover." → Neutral
    22 "Paul, an apostle of Jesus Christ, by the will of God, to the saints in Ephesus." → Neutral
    23 "And the Lord spoke to Moses, saying, 'Command the Israelites to bring you clear oil for the lampstand.'" → Neutral

    Now analyze this text:
    "{}" →
    """.format(text)

    response = together.Complete.create(
        model="mistralai/Mistral-7B-Instruct-v0.2",
        prompt=prompt,
        max_tokens=10
    )
    return response["choices"][0]["text"].strip()

def get_sentiment(response) -> int:
    response = response.lower()
    if response == "negative":
        print("sentiment = -")
        return -1
    elif response == "postitive":
        print("sentiment = +")
        return 1
    elif response == "nuetral":
        print("sentiment = n")
        return 0
    else:
        return 0

print(get_sentiment("Negative"))
print(get_sentiment("Nuetral"))
print(get_sentiment("postitive"))

key_books_indicies = [book_order[book] for book in key_books]
print(key_books_indicies)

import random
import contextlib
import matplotlib.pyplot as plt

# Analyze the sentiment for selected books across translations using the few_shot function.
def analyze_translation_sentiment(bible_translations, selected_books, sentiment_func, sample_size=5):
    """
    Returns:
        dict: Mapping of translation name to its average sentiment score (over selected books)
              using a random sample of verses per book.
    """
    # First, compute the common (chapter, verse) positions for each selected book across all translations.
    common_verse_positions = {}
    for book in selected_books:
        common_positions = None
        for translation, books in bible_translations.items():
            if book not in books:
                common_positions = set()  # If one translation lacks the book, no common verses.
                break
            positions = set()
            for chapter_key, chapter in books[book].items():
                for verse_key in chapter.keys():
                    positions.add((chapter_key, verse_key))
            if common_positions is None:
                common_positions = positions
            else:
                common_positions = common_positions.intersection(positions)
        if common_positions is None:
            common_positions = set()
        common_verse_positions[book] = list(common_positions)

    # Now, randomly sample n verses for each book from the common positions.
    sampled_positions = {}
    for book, positions in common_verse_positions.items():
        if not positions:
            sampled_positions[book] = []
        else:
            n = min(sample_size, len(positions))
            sampled_positions[book] = random.sample(positions, n)

    translation_scores = {}

    # Process each translation using the same sampled verse positions per book.
    for translation, books in bible_translations.items():
        scores = []
        for book in selected_books:
            if book not in books:
                continue
            for (chapter_key, verse_key) in sampled_positions.get(book, []):
                chapter = books[book].get(chapter_key, {})
                verse_text = chapter.get(verse_key)
                if verse_text is None:
                    continue
                sentiment = sentiment_func(verse_text)
                if sentiment.lower() == "negative":
                    score = -1
                elif sentiment.lower() == "positive":
                    score = 1
                else:
                    score = 0
                print(translation + str(score) + " = " + verse_text)
                scores.append(score)
        if scores:
            average_score = sum(scores) / len(scores)
            translation_scores[translation] = average_score
    return translation_scores

translation_scores = analyze_translation_sentiment(bible_dict, key_books_indicies, few_shot_sentiment, 100)

years = []
scores = []
labels = []
for translation, score in translation_scores.items():
    year = translation_years.get(translation)
    if year is not None:
        years.append(year)
        scores.append(score)
        labels.append(translation)

plt.figure(figsize=(8, 5))
plt.scatter(years, scores, s=100, label="Average Sentiment")

for i, label in enumerate(labels):
    plt.annotate(label, (years[i], scores[i]), textcoords="offset points", xytext=(5, 5), ha='center')

coeffs = np.polyfit(years, scores, 1)  # slope and intercept
x_fit = np.linspace(min(years), max(years), 100)
y_fit = np.polyval(coeffs, x_fit)

plt.plot(x_fit, y_fit, color='red', label="Line of Best Fit")

plt.xlabel("Publication Year")
plt.ylabel("Average Sentiment Score")
plt.title("Sentiment Analysis of Bible Translations (Few-shot)")
plt.grid(True)
plt.legend()
plt.show()

print(translation_scores)

verses = [
    "For the Son of man is come to seek and to save that which was lost.",
    "For God so loved the world, that he gave his only begotten Son.",
    "Jesus wept.",
    "Happy is the one who seizes your infants and dashes them against the rocks"
]

for verse in verses:
    zero_shot = zero_shot_sentiment(verse)
    few_shot = few_shot_sentiment(verse)

    print(f"Verse: {verse}")
    print(f"Zero-Shot Prediction: {zero_shot}")
    print(f"Few-Shot Prediction: {few_shot}")
    print("-" * 50)

verses = [
    "And if a man smite his servant, or his maid, with a rod, and he die under his hand; he shall be surely punished. Notwithstanding, if he continue a day or two, he shall not be punished: for he is his money.",
    "At daybreak the woman went back to the house where her master was staying, fell down at the door and lay there until daylight.",
    "When he reached home, he took a knife and cut up his concubine, limb by limb, into twelve parts and sent them into all the areas of Israel.",
]

for verse in verses:
    zero_shot = zero_shot_sentiment(verse)
    few_shot = few_shot_sentiment(verse)

    print(f"Verse: {verse}")
    print(f"Zero-Shot Prediction: {zero_shot}")
    print(f"Few-Shot Prediction: {few_shot}")
    print("-" * 50)

verses = [
    "And whatever you ask in my name, I will do, so that the Father may be glorified in the Son.  If you ask anything of me in my name, I will do it",
    "The Lord gave, and the Lord has taken away; blessed be the name of the Lord.",
    "To everything there is a season... A time to be born, and a time to die.",
    "Do not think that I have come to bring peace to the earth. I have not come to bring peace, but a sword.",
    "Faithful are the wounds of a friend, but deceitful are the kisses of an enemy.",
    "For when I am weak, then I am strong.",
    "The greatest among you shall be your servant. Whoever exalts himself will be humbled, and whoever humbles himself will be exalted.",
    "Blessed shall he be who takes your little ones and dashes them against the rock!",
    "And we know that all things work together for good to those who love God.",
    "If anyone comes to me and does not hate his father and mother, wife and children, brothers and sisters—yes, even their own life—such a person cannot be my disciple.",
    "Consider it pure joy, my brothers and sisters, whenever you face trials of many kinds."
]

for verse in verses:
    zero_shot = zero_shot_sentiment(verse)
    few_shot = few_shot_sentiment(verse)

    print(f"Verse: {verse}")
    print(f"Zero-Shot Prediction: {zero_shot}")
    print(f"Few-Shot Prediction: {few_shot}")
    print("-" * 50)

"""# Training a model for translation detection"""

import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

print(bible_dict)

def get_verses_with_labels(bible_dict):
    """
    Returns two lists: texts (the verses), labels (which translation).
    """
    texts = []
    labels = []

    for translation, books_dict in bible_dict.items():
        for book, chapters_dict in books_dict.items():
            for chapter, verses_dict in chapters_dict.items():
                for verse_num, verse_text in verses_dict.items():
                    # Basic text cleaning (do not remove archaic language(?))
                    text = verse_text.lower()
                    text = re.sub(r"[^\w\s]", "", text)  # remove punctuation
                    text = text.strip()

                    texts.append(text)
                    labels.append(translation)
    return texts, labels

texts, labels = get_verses_with_labels(bible_dict)

labeled_verses = get_verses_with_labels(bible_dict)

translation_set = list(set(labels))  # unique translation names
translation_to_idx = {t: i for i, t in enumerate(translation_set)}

y = np.array([translation_to_idx[label] for label in labels])

X_train_texts, X_test_texts, y_train, y_test = train_test_split(
    texts, y, test_size=0.2, random_state=42, stratify=y
)

# Hyperparameters
MAX_VOCAB_SIZE = 100000  # maximum number of words in vocabulary
MAX_SEQ_LEN = 50        # maximum number of tokens per verse (pad or truncate)

# Boilerplate
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<UNK>")
tokenizer.fit_on_texts(X_train_texts)

X_train_seq = tokenizer.texts_to_sequences(X_train_texts)
X_test_seq = tokenizer.texts_to_sequences(X_test_texts)

X_train = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
X_test = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

num_classes = len(translation_set)

y_train_categorical = to_categorical(y_train, num_classes=num_classes)
y_test_categorical = to_categorical(y_test, num_classes=num_classes)

vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
embedding_dim = 128  # dimension of tok embeddings

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    input_length=MAX_SEQ_LEN))
model.add(LSTM(64, return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Hyperparameters
vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
embedding_dim = 128  # Reduced embedding dimension for speed
max_seq_length = MAX_SEQ_LEN

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    input_length=max_seq_length))
model.add(Dropout(0.4))
# Use fewer units and remove recurrent_dropout to leverage CuDNN optimizations.
model.add(Bidirectional(LSTM(64, return_sequences=False, dropout=0.3)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(num_classes, activation='softmax'))

optimizer = Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Assume MAX_VOCAB_SIZE, tokenizer, MAX_SEQ_LEN, num_classes are defined
vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
embedding_dim = 256  # Increased dimension for richer embeddings

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    input_length=MAX_SEQ_LEN))
# Bidirectional LSTM layers to capture context from both directions.
# Removed recurrent_dropout (set to 0) to allow for CuDNN optimizations.
model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.0)))
model.add(Bidirectional(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.0)))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Increase learning rate slightly for faster training.
optimizer = Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model.summary()

batch_size = 32
epochs = 10  # Increase(?)

history = model.fit(
    X_train,
    y_train_categorical,
    validation_split=0.2,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

from sklearn.metrics import classification_report

test_loss, test_acc = model.evaluate(X_test, y_test_categorical, verbose=0)
print("Test Accuracy:", test_acc)
print("Test Loss:", test_loss)

pred_probs = model.predict(X_test)
predicted_labels = np.argmax(pred_probs, axis=1)

print(classification_report(y_test, predicted_labels, target_names=translation_set))

import re
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

def predict_translation(verse_text, model, tokenizer, translation_set, MAX_SEQ_LEN=50):
    """
    Given a single verse_text string, predict which translation
    it most likely comes from.

    Returns the predicted translation name.
    """
    text_clean = verse_text.lower()
    text_clean = re.sub(r"[^\w\s]", "", text_clean)
    text_clean = text_clean.strip()

    seq = tokenizer.texts_to_sequences([text_clean])

    seq_padded = pad_sequences(seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

    pred_probs = model.predict(seq_padded) # shape
    pred_label_idx = np.argmax(pred_probs, axis=1)[0] # highest probability
    pred_translation = translation_set[pred_label_idx] # map index to translation

    return pred_translation

sample_verse = "In the beginning God created the heaven and the earth."
predicted = predict_translation(
    verse_text=sample_verse,
    model=model,
    tokenizer=tokenizer,
    translation_set=translation_set,
    MAX_SEQ_LEN=50
)
print("Predicted translation:", predicted)

"""# Better classifier with better strategy"""

def flatten_and_merge_verses(bible_dict, min_tokens=20):
    """
    Flattens a nested 'bible_dict' and merges consecutive verses
    until the text has at least 'min_tokens' words.
    """
    all_data = []

    for translation, books in bible_dict.items():
        for book_idx, chapters in books.items():
            for chapter_idx, verses_dict in chapters.items():
                verse_numbers = sorted(verses_dict.keys())
                i = 0

                while i < len(verse_numbers):
                    # Start with the current verse text
                    merged_text = verses_dict[verse_numbers[i]].strip()
                    current_tokens = merged_text.split()
                    i += 1

                    # Merge with subsequent verses until minimum tokens are reached or no more verses
                    while len(current_tokens) < min_tokens and i < len(verse_numbers):
                        next_verse_text = verses_dict[verse_numbers[i]].strip()
                        merged_text = merged_text + " " + next_verse_text
                        current_tokens = merged_text.split()
                        i += 1

                    # Now merged_text has at least 'min_tokens' words or we're out of verses
                    all_data.append((merged_text, translation))

    return all_data

flat_bible_dict = flatten_and_merge_verses(bible_dict)

print(flat_bible_dict)

import random
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

from collections import defaultdict

def remove_texts_in_multiple_translations(all_data):
    """
    Removes any verse text that appears (exact match) in more than one translation.
    For example, if the same text is found in KJV and NIV, remove it entirely
    from the dataset, so only texts unique to one translation remain.
    """
    # Map each text -> set of translations in which it appears
    text_to_translations = defaultdict(set)

    for text, translation in all_data:
        text_to_translations[text].add(translation)

    final_data = []
    for text, translation in all_data:
        if len(text_to_translations[text]) == 1:
            final_data.append((text, translation))

    return final_data

all_data = remove_texts_in_multiple_translations(flat_bible_dict)

translation_labels = list({label for (_, label) in all_data})
translation_to_id = {label: idx for idx, label in enumerate(translation_labels)}

num_classes = len(translation_labels)
print("Translation mapping:", translation_to_id)

random.shuffle(all_data)

train_size = int(0.8 * len(all_data))
val_size = int(0.1 * len(all_data))
test_size = len(all_data) - train_size - val_size

train_data = all_data[:train_size]
val_data   = all_data[train_size:train_size + val_size]
test_data  = all_data[train_size + val_size:]

from collections import Counter

def tokenize(text):
    return text.lower().split()

def build_vocab(dataset):
    """
    dataset: list of (text, label) tuples
    returns: (word2idx dict, idx2word list)
    """
    counter = Counter()
    for text, _ in dataset:
        tokens = tokenize(text)
        counter.update(tokens)

    # Create word2idx
    # Reserve 0 for <PAD>, 1 for <UNK>
    word2idx = {"<PAD>": 0, "<UNK>": 1}
    for word, freq in counter.items():
        word2idx[word] = len(word2idx)

    idx2word = [None] * len(word2idx)
    for w, i in word2idx.items():
        idx2word[i] = w

    return word2idx, idx2word

word2idx, idx2word = build_vocab(train_data)
vocab_size = len(word2idx)
print("Vocab size:", vocab_size)

class BibleDataset(Dataset):
    def __init__(self, data, word2idx, translation_to_id):
        super().__init__()
        self.data = data
        self.word2idx = word2idx
        self.translation_to_id = translation_to_id

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, label_str = self.data[idx]

        label_id = self.translation_to_id[label_str]
        tokens = tokenize(text)
        input_ids = [self.word2idx.get(t, self.word2idx["<UNK>"]) for t in tokens]

        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(label_id, dtype=torch.long)

def collate_fn(batch):
    """
    batch: list of (input_ids, label_id) from __getitem__
    Returns: (padded_input_ids, lengths, label_ids)
    """
    input_ids_list, label_list = zip(*batch)  # unzip

    # Pad the sequences
    lengths = [len(seq) for seq in input_ids_list]
    padded_input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=0)  # 0 is <PAD>

    label_ids = torch.stack(label_list)

    return padded_input_ids, torch.tensor(lengths), label_ids

# Datasets
train_dataset = BibleDataset(train_data, word2idx, translation_to_id)
val_dataset   = BibleDataset(val_data,   word2idx, translation_to_id)
test_dataset  = BibleDataset(test_data,  word2idx, translation_to_id)

BATCH_SIZE = 16

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=collate_fn)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()

        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, input_ids, lengths):
        """
        input_ids: (batch_size, seq_len)
        lengths: (batch_size,) - actual lengths of each sequence (un-padded)
        Returns: logits (batch_size, num_classes)
        """
        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embed_dim)

        packed_input = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)

        packed_output, (hidden, cell) = self.lstm(packed_input)

        out = hidden.squeeze(0)  # (batch_size, hidden_dim)

        # Final linear
        logits = self.fc(out)

        return logits

EMBED_DIM = 128
HIDDEN_DIM = 256

model = LSTMClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def compute_accuracy(logits, labels):
    """
    logits: (batch_size, num_classes)
    labels: (batch_size,)
    """
    preds = torch.argmax(logits, dim=1)
    correct = (preds == labels).sum().item()
    total = labels.size(0)
    return correct / total

def train_one_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    total_acc = 0.0
    total_samples = 0

    for batch in train_loader:
        input_ids, lengths, labels = batch
        input_ids = input_ids.to(device)
        lengths = lengths.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        logits = model(input_ids, lengths)
        loss = criterion(logits, labels)

        loss.backward()
        optimizer.step()

        batch_size = labels.size(0)
        acc = compute_accuracy(logits, labels)

        total_loss += loss.item() * batch_size
        total_acc += acc * batch_size
        total_samples += batch_size

    epoch_loss = total_loss / total_samples
    epoch_acc = total_acc / total_samples
    return epoch_loss, epoch_acc

def evaluate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_acc = 0.0
    total_samples = 0

    with torch.no_grad():
        for batch in val_loader:
            input_ids, lengths, labels = batch
            input_ids = input_ids.to(device)
            lengths = lengths.to(device)
            labels = labels.to(device)

            logits = model(input_ids, lengths)
            loss = criterion(logits, labels)

            batch_size = labels.size(0)
            acc = compute_accuracy(logits, labels)

            total_loss += loss.item() * batch_size
            total_acc += acc * batch_size
            total_samples += batch_size

    epoch_loss = total_loss / total_samples
    epoch_acc = total_acc / total_samples
    return epoch_loss, epoch_acc

# Main training loop
EPOCHS = 10

for epoch in range(EPOCHS):
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)

    print(f"Epoch {epoch+1}/{EPOCHS}")
    print(f"  Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}")
    print(f"  Val loss:   {val_loss:.4f}   | Val acc:   {val_acc:.4f}")

test_loss, test_acc = evaluate(model, test_loader, criterion, device)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}")

from sklearn.metrics import confusion_matrix

def compute_confusion_matrix_sklearn(model, data_loader, device):
    all_preds = []
    all_labels = []

    model.eval()
    with torch.no_grad():
        for batch in data_loader:
            input_ids, lengths, labels = batch
            input_ids = input_ids.to(device)
            lengths = lengths.to(device)
            labels = labels.to(device)

            logits = model(input_ids, lengths)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return confusion_matrix(all_labels, all_preds)

cm = compute_confusion_matrix_sklearn(model, test_loader, device)
print("Confusion Matrix:\n", cm)

import matplotlib.pyplot as plt
import numpy as np

def plot_confusion_matrix(cm, label_names):
    fig, ax = plt.subplots()
    cax = ax.matshow(cm, cmap=plt.cm.Reds)
    fig.colorbar(cax)

    ax.set_xticks(np.arange(len(label_names)))
    ax.set_yticks(np.arange(len(label_names)))
    ax.set_xticklabels(label_names, rotation=45, ha="left")
    ax.set_yticklabels(label_names)
    # Adding text in the middle of the squares
    for i in range(len(label_names)):
        for j in range(len(label_names)):
            ax.text(j, i, f"{cm[i, j]}", ha='center', va='center', color='black')

    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    plt.show()

plot_confusion_matrix(cm, translation_labels)

all_data_no_kjv = [(text, trans) for (text, trans) in all_data if trans != "t_kjv"]

import random

random.shuffle(all_data_no_kjv)

train_size = int(0.8 * len(all_data_no_kjv))
val_size = int(0.1 * len(all_data_no_kjv))
test_size = len(all_data_no_kjv) - train_size - val_size

train_data = all_data_no_kjv[:train_size]
val_data   = all_data_no_kjv[train_size:train_size + val_size]
test_data  = all_data_no_kjv[train_size + val_size:]

translation_labels = list({label for (_, label) in all_data_no_kjv})
translation_to_id = {label: idx for idx, label in enumerate(translation_labels)}
num_classes = len(translation_labels)

print("Translations used (no KJV):", translation_to_id)

train_dataset = BibleDataset(train_data, word2idx, translation_to_id)
val_dataset   = BibleDataset(val_data,   word2idx, translation_to_id)
test_dataset  = BibleDataset(test_data,  word2idx, translation_to_id)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=collate_fn)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EMBED_DIM = 128
HIDDEN_DIM = 256

model = LSTMClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

EPOCHS = 10
for epoch in range(EPOCHS):
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)

    print(f"Epoch {epoch+1}/{EPOCHS}")
    print(f"  Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}")
    print(f"  Val loss:   {val_loss:.4f}   | Val acc:   {val_acc:.4f}")

test_loss, test_acc = evaluate(model, test_loader, criterion, device)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}")

cm = compute_confusion_matrix_sklearn(model, test_loader, device)
plot_confusion_matrix(cm, translation_labels)

all_data_no_kjv_or_wbt = [(text, trans) for (text, trans) in all_data_no_kjv if trans != "t_wbt"]

import random

random.shuffle(all_data_no_kjv_or_wbt)

train_size = int(0.8 * len(all_data_no_kjv_or_wbt))
val_size = int(0.1 * len(all_data_no_kjv_or_wbt))
test_size = len(all_data_no_kjv_or_wbt) - train_size - val_size

train_data = all_data_no_kjv_or_wbt[:train_size]
val_data   = all_data_no_kjv_or_wbt[train_size:train_size + val_size]
test_data  = all_data_no_kjv_or_wbt[train_size + val_size:]

translation_labels = list({label for (_, label) in all_data_no_kjv_or_wbt})
translation_to_id = {label: idx for idx, label in enumerate(translation_labels)}
num_classes = len(translation_labels)

print("Translations used (no KJV or WBT):", translation_to_id)

train_dataset = BibleDataset(train_data, word2idx, translation_to_id)
val_dataset   = BibleDataset(val_data,   word2idx, translation_to_id)
test_dataset  = BibleDataset(test_data,  word2idx, translation_to_id)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=collate_fn)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                          collate_fn=collate_fn)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EMBED_DIM = 128
HIDDEN_DIM = 256

model = LSTMClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)


EPOCHS = 10
for epoch in range(EPOCHS):
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)

    print(f"Epoch {epoch+1}/{EPOCHS}")
    print(f"  Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}")
    print(f"  Val loss:   {val_loss:.4f}   | Val acc:   {val_acc:.4f}")

test_loss, test_acc = evaluate(model, test_loader, criterion, device)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}")

cm = compute_confusion_matrix_sklearn(model, test_loader, device)
plot_confusion_matrix(cm, translation_labels)
